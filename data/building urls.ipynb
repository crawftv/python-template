{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = open(\"realpython_urls.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = re.findall(r'(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+',s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"saved_thing\"\n",
    "outfile = open(\"clean_real_python.txt\",\"wb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "s=open(\"dev_to_sitemap.xml\")\n",
    "s = s.read()\n",
    "t = re.findall(r'(?:(?:https?|ftp):\\/\\/)?[\\w/\\-?=%.]+\\.[\\w/\\-?=%.]+',s)\n",
    "outfile = open(\"clean_dev_to_sitemap\",\"wb\")\n",
    "pickle.dump(t, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_urls(string):\n",
    "    if \"dev.to\" in string:\n",
    "        if \"hi-im\" not in string:\n",
    "            return string\n",
    "tt = [get_urls(i) for i in t]\n",
    "tt = [i for i in tt if i is not None]\n",
    "outfile = open(\"clean_dev_to_sitemap\",\"wb\")\n",
    "pickle.dump(tt, outfile)\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# possible keywords\n",
    "django, javascript, lisp, emacs, clojure, clojurescript, react, swift, vim, go, c++, linux, flask, angular, html, css, bootstrap, json, postgres, ruby, rails, materialui, git, ci, travis-ci, circleci, aws, mongodb, c, interviews, vr, rust, elm, tdd, oop, functional-programming, algorithms, typescript, android, node, kotlin, oauth, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45075"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = \"clean_dev_to_sitemap\"\n",
    "infile = open(filename, \"rb\")\n",
    "urls = pickle.load(infile)\n",
    "infile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = requests.get(\"https://dev.to/googlecloud/running-multiple-versions-of-python-in-cloud-run-4hl8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(a.text,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = soup.find_all(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l =\" \".join([i.text for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "code = soup.find_all(\"code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'$ python37 recommendations.py\\n  File \"recommendations.py\", line 42\\n    while (chunk := stream.read(256)) != \\'\\':\\n                 ^\\nSyntaxError: invalid syntax\\n FROM python:2.7\\n...\\n FROM python:3.8\\n...\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([i.text for i in code])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect(\"inbrain.db\") as conn:\n",
    "    conn.execute(\"CREATE TABLE scraped_pages (url text PRIMARY KEY, page_text text, page_code text )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_pages(url, results):\n",
    "    r = requests.get(url).text\n",
    "    soup = BeautifulSoup(r, \"html.parser\")\n",
    "    page_text = soup.find_all(\"p\")\n",
    "    page_text = \" \".join([i.text for i in page_text])\n",
    "    page_code = soup.find_all(\"c\")\n",
    "    page_code = \" \".join([i.text for i in page_code])\n",
    "    results.append( (url,page_text,page_code)  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import requests\n",
    "import nest_asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "async def create_scrape_loop(\n",
    "    iterable, individual_scrape_function, num_workers=40, *args\n",
    "):\n",
    "    \"\"\"This function is responsible for organizing the asyncrhonous pieces.\n",
    "    It creates a session, uses a list comprehension to create the tasks,\n",
    "    and gathers the tasks.\n",
    "    Parameters\n",
    "    ----------\n",
    "    iterable : list, default = None\n",
    "        This a list of things to repeat with the individual_scrape_function.\n",
    "        It can be, for example, a list of urls or page numbers.\n",
    "    individual_scrape_function : function, default = None\n",
    "        A function designed for scraping a single thing, usually a page or api request.\n",
    "        Takes an element from the iterable and uses that to do multiple operations.\n",
    "    num_workers : <class 'int'>, default = 40\n",
    "        #TODO description\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "        with requests.Session() as session:\n",
    "            loop = asyncio.get_event_loop()\n",
    "            tasks = [\n",
    "                loop.run_in_executor(executor, individual_scrape_function, *(i, *args))\n",
    "                for i in iterable\n",
    "            ]\n",
    "            for response in await asyncio.gather(*tasks):\n",
    "                pass\n",
    "\n",
    "\n",
    "def async_web_scrape(iterable, individual_scrape_function, num_workers, *args):\n",
    "    \"\"\"This function takes a function designed to scrape a single thing,\n",
    "    i.e. a web page or api request. The individual_scrape_function gets\n",
    "    passed to the the create_loop_function with the iterable and other args.\n",
    "    Parameters\n",
    "    ----------\n",
    "    iterable : list, default = None\n",
    "        This a list of things to repeat with the individual_scrape_function.\n",
    "        It can be, for example, a list of urls or page numbers.\n",
    "    individual_scrape_function : function, default = None\n",
    "        A function designed for scraping a single thing, usually a page or api request.\n",
    "        Takes an element from the iterable and uses that to do multiple operations.\n",
    "    num_workers : <class 'int'>, default = 40\n",
    "        The number of workers for the thread. This is passed directly to the \n",
    "        create_scrape_loop function.\n",
    "    *args : these are the parameters of the individual scrape function.\n",
    "    Returns\n",
    "    -------\n",
    "        Nothing. Unfortunately, this happens to use side effects. \n",
    "        Maybe in the future this can be changed.\n",
    "    \"\"\"\n",
    "    future = asyncio.ensure_future(\n",
    "        create_scrape_loop(iterable, individual_scrape_function, num_workers, *args)\n",
    "    )\n",
    "    loop = asyncio.get_event_loop()\n",
    "    loop.run_until_complete(future)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "for i in range(3,47):\n",
    "    print(i)\n",
    "    search = urls[i*1_000:(i+1)*1_000]\n",
    "    results = []\n",
    "    async_web_scrape(search,scrape_pages,40,results)\n",
    "    \n",
    "    \n",
    "    with sqlite3.connect(\"inbrain.db\") as conn:\n",
    "        conn.executemany(\"INSERT INTO scraped_pages VALUES (?,?,?)\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45074,)\n"
     ]
    }
   ],
   "source": [
    "with sqlite3.connect(\"inbrain.db\") as conn:\n",
    "    count = conn.execute(\"SELECT COUNT(url) from scraped_pages\").fetchone()\n",
    "    print(count)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
